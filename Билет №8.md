Метод наименьших квадратов (МНК). Постановка задачи аппроксимации, критерий минимизации ошибки, вывод системы нормальных уравнений (на примере линейной зависимости).
![[Pasted image 20251224154336.png]]
![[Pasted image 20251224154353.png]]
![[Pasted image 20251224154707.png]]
# Билет №8: Метод наименьших квадратов (МНК)

**Содержание:**
1. Постановка задачи аппроксимации.
2. Критерий минимизации.
3. Вывод системы нормальных уравнений (линейный случай).
4. Обоснование единственности решения (неравенство Коши-Буняковского).
5. Общий случай (полиномиальная аппроксимация).

---

### 1. Постановка задачи

Даны точки $(x_i, y_i), \, i=\overline{1,n}$.
Требуется построить функцию (в простейшем случае — прямую $y = ax + b$), которая "наилучшим образом" проходит через эти точки.

**Критерий качества:**
Сумма квадратов отклонений экспериментальных данных от теоретических должна быть минимальна:
```math
J = \sum_{i=1}^n (y_i - (ax_i + b))^2 \to \min
```
Надо минимизировать функционал $J$.

---

### 2. Вывод нормальных уравнений (для прямой $y=ax+b$)

Функционал $J$ зависит от 2 переменных: $a$ и $b$ (параметры прямой).
Чтобы найти минимум, запишем частные производные по этим параметрам и приравняем их к нулю:

```math
\begin{cases}
J'_a = \sum_{i=1}^n 2(y_i - ax_i - b) \cdot (-x_i) = 0 \\
J'_b = \sum_{i=1}^n 2(y_i - ax_i - b) \cdot (-1) = 0
\end{cases}
```

Раскроем скобки и сократим на $-2$:
```math
\begin{cases}
\sum (y_i x_i - a x_i^2 - b x_i) = 0 \\
\sum (y_i - a x_i - b) = 0
\end{cases}
```

Перенесем известные величины ($y_i$) в одну сторону, а неизвестные ($a, b$) в другую. Получаем **СЛУ (Систему Линейных Уравнений)** относительно $a$ и $b$:
```math
\begin{cases}
a \sum_{i=1}^n x_i^2 + b \sum_{i=1}^n x_i = \sum_{i=1}^n x_i y_i \\
a \sum_{i=1}^n x_i + b \cdot n = \sum_{i=1}^n y_i
\end{cases}
```
*(Здесь $\sum b = b \cdot n$)*.

В матричном виде:
```math
\begin{pmatrix}
\sum x_i^2 & \sum x_i \\
\sum x_i & n
\end{pmatrix}
\cdot
\begin{pmatrix}
a \\
b
\end{pmatrix}
=
\begin{pmatrix}
\sum x_i y_i \\
\sum y_i
\end{pmatrix}
```

---

### 3. Обоснование разрешимости

Данная система всегда имеет решение, если определитель матрицы не равен нулю.
```math
\Delta = \left| \begin{matrix} \sum x_i^2 & \sum x_i \\ \sum x_i & n \end{matrix} \right| = n \cdot \sum x_i^2 - (\sum x_i)^2
```
Нужно показать, что $n \cdot \sum x_i^2 > (\sum x_i)^2$.

**Применим Неравенство Коши-Буняковского:**
Пусть $X, Y$ — вектора из $\mathbb{R}^n$.
```math
(X, Y)^2 \le \|X\|^2 \cdot \|Y\|^2 = (X, X) \cdot (Y, Y)
```

Возьмем:
*   Вектор $X = (x_1, x_2, \dots, x_n)$
*   Вектор $Y = (1, 1, \dots, 1)$

Тогда:
*   $(X, Y) = \sum x_i \cdot 1 = \sum x_i$
*   $(X, X) = \sum x_i^2$
*   $(Y, Y) = \sum 1^2 = n$

Подставляем в неравенство:
```math
(\sum x_i)^2 \le (\sum x_i^2) \cdot n
```
Равенство достигается только если вектора коллинеарны (т.е. все $x_i$ равны, что на практике не бывает).
$\Rightarrow \Delta > 0$, система имеет единственное решение.

---

### 4. Обобщение (Полиномиальная аппроксимация)

В более общем случае мы будем искать не прямую, а полином степени $m < (n-1)$.
Можно получить выражение $X\vec{a} = \vec{z}$, где $X$ — матрица коэффициентов нормальной системы:

$$ X = \begin{pmatrix}
\sum x_i^{2m} & \dots & \sum x_i^m \\
\vdots & \ddots & \vdots \\
\sum x_i^m & \dots & \sum x_i^0 (=n)
\end{pmatrix} ```math

``` \vec{a} = \begin{pmatrix} a_m \\ \vdots \\ a_0 \end{pmatrix}, \quad
\vec{z} = \begin{pmatrix} \sum x_i^m y_i \\ \vdots \\ \sum y_i \end{pmatrix} $$
